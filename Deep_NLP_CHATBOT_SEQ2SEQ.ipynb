{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep-NLP-CHATBOT-SEQ2SEQ",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMuqLXmoaLSy9JXr5Zi6sQ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyake/NLP/blob/master/Deep_NLP_CHATBOT_SEQ2SEQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XESvT8fqe1BM",
        "outputId": "1cef76d4-70c5-4458-90a1-5d49fa9c3171"
      },
      "source": [
        "import re \r\n",
        "import numpy as np \r\n",
        "import tensorflow as tf\r\n",
        "import time\r\n",
        "\r\n",
        "\r\n",
        "lines=open('/content/movie_lines.txt',encoding='utf8',errors='ignore').read().split('\\n')\r\n",
        "conversations=open('/content/movie_conversations.txt',encoding='utf8',errors='ignore').read().split('\\n')\r\n",
        "#creating a dictionary that maps each line and its id\r\n",
        "id2line={}\r\n",
        "for line in lines:\r\n",
        "    line1=line.split(' +++$+++ ')\r\n",
        "    if len(line1)==5:\r\n",
        "        id2line[line1[0]]=line1[4]\r\n",
        "        \r\n",
        "#creating a list of all the convesations #data cleaning\r\n",
        "\r\n",
        "conversations_ids=[]\r\n",
        "for conversation in conversations[:-1]: #exclude last row \r\n",
        "    conversation1=conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\") #remove single quotes by nothing    \r\n",
        "    conversations_ids.append(conversation1.split(','))\r\n",
        "#getting seperately the questions and the answers\r\n",
        "    #questions will be inputs and answers will be targets.\r\n",
        "    \r\n",
        "    #looping through the dictionary and conversation ids\r\n",
        "questions=[]\r\n",
        "answers=[]\r\n",
        "for conversation in conversations_ids:\r\n",
        "    for i in range(len(conversation)-1):\r\n",
        "        questions.append(id2line[conversation[i]])\r\n",
        "        answers.append(id2line[conversation[i+1]])\r\n",
        "def clean_text(text):\r\n",
        "    text = text.lower()\r\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\r\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\r\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\r\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\r\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\r\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\r\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\r\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\r\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\r\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\r\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\r\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\r\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\r\n",
        "    return text\r\n",
        " \r\n",
        "# Cleaning the questions\r\n",
        "clean_questions = []\r\n",
        "for question in questions:\r\n",
        "    clean_questions.append(clean_text(question))\r\n",
        " \r\n",
        "# Cleaning the answers\r\n",
        "clean_answers = []\r\n",
        "for answer in answers:\r\n",
        "    clean_answers.append(clean_text(answer))\r\n",
        " \r\n",
        "# Creating a dictionary that maps each word to its number of occurrences\r\n",
        "word2count = {}\r\n",
        "for question in clean_questions:\r\n",
        "    for word in question.split():\r\n",
        "        if word not in word2count:\r\n",
        "            word2count[word] = 1\r\n",
        "        else:\r\n",
        "            word2count[word] += 1\r\n",
        "for answer in clean_answers:\r\n",
        "    for word in answer.split():\r\n",
        "        if word not in word2count:\r\n",
        "            word2count[word] = 1\r\n",
        "        else:\r\n",
        "            word2count[word] += 1\r\n",
        " \r\n",
        "# Creating two dictionaries that map the questions words and the answers words to a unique integer\r\n",
        "threshold_questions = 20\r\n",
        "questionswords2int = {}\r\n",
        "word_number = 0\r\n",
        "for word, count in word2count.items():\r\n",
        "    if count >= threshold_questions:\r\n",
        "        questionswords2int[word] = word_number\r\n",
        "        word_number += 1\r\n",
        "threshold_answers = 20\r\n",
        "answerswords2int = {}\r\n",
        "word_number = 0\r\n",
        "for word, count in word2count.items():\r\n",
        "    if count >= threshold_answers:\r\n",
        "        answerswords2int[word] = word_number\r\n",
        "        word_number += 1\r\n",
        "tokens=['<PAD>','<EOS>','<OUT>','<SOS>']\r\n",
        "for token in tokens:\r\n",
        "    questionswords2int[token]=len(questionswords2int)+1\r\n",
        "    \r\n",
        "for token in tokens:\r\n",
        "    answerswords2int[token]=len(answerswords2int)+1\r\n",
        "#create inversemapping dictionary\r\n",
        "answersints2word={w_i:w for w,w_i in answerswords2int.items()}   \r\n",
        "\r\n",
        "#adding the end of string token to the end of every answer\r\n",
        "for i in range(len(clean_answers)):\r\n",
        "    clean_answers[i]+=' <EOS>'\r\n",
        "#Translating all questions and the answers to integers \r\n",
        "#and replace all the words that were filtered out by <OUT>\r\n",
        "    # if word is not in clean _questions replace by <out> and assign the corresponding integer\r\n",
        "    \r\n",
        "questions_to_int=[]\r\n",
        "for question in clean_questions:\r\n",
        "    ints=[]\r\n",
        "    for words in question.split():\r\n",
        "        if word not in questionswords2int:\r\n",
        "            ints.append(questionswords2int['<OUT>'])\r\n",
        "        else:\r\n",
        "            ints.append(questionswords2int[word])\r\n",
        "    questions_to_int.append(ints)\r\n",
        "            \r\n",
        "answers_to_int=[]\r\n",
        "for answer in clean_answers:\r\n",
        "    ints=[]\r\n",
        "    for words in answer.split():\r\n",
        "        if word not in answerswords2int:\r\n",
        "            ints.append(answerswords2int['<OUT>'])\r\n",
        "        else:\r\n",
        "            ints.append(answerswords2int[word])\r\n",
        "    answers_to_int.append(ints)\r\n",
        " #sorting questions ans answers by length of the questions to speed up training\r\n",
        "sorted_clean_questions=[]\r\n",
        "sorted_clean_answers=[]\r\n",
        "for length in range(1,55+1):\r\n",
        "    for i in enumerate(questions_to_int):\r\n",
        "        if len(i[1])==length:\r\n",
        "            sorted_clean_questions.append(questions_to_int[i[0]])\r\n",
        "            sorted_clean_answers.append(answers_to_int[i[0]])\r\n",
        "     #########END OF PREPROCESSING#####################       \r\n",
        "     \r\n",
        "\r\n",
        "     \r\n",
        "          \r\n",
        "     \r\n",
        "def model_inputs():\r\n",
        "    inputs=tf.placeholder(tf.int32,[None,None],name='input')\r\n",
        "    targets=tf.placeholder(tf.int32,[None,None],name='target')\r\n",
        "    lr=tf.placeholder(tf.float32,name='learning_rate')\r\n",
        "    keep_prob=tf.placeholder(tf.float32,name='keep_prob')\r\n",
        "    return inputs,targets,lr,keep_prob\r\n",
        "def preprocess_targets(targets,word2int,batch_size):\r\n",
        "    left_side=tf.fill([batch_size,1],word2int['<SOS>'])\r\n",
        "    right_side=tf.strided_slice(targets, [0,0], [batch_size,-1], [1,1])\r\n",
        "    preprocessed_targets=tf.concat([left_side,right_side],1)\r\n",
        "    return preprocessed_targets\r\n",
        "#create ENCODER RNN layer \r\n",
        "def encoder_rnn_layer(rnn_inputs,rnn_size,num_layers,keep_prob,sequence_length):\r\n",
        "    lstm=tf.contrib.rnn.BasicLSTMCell(rnn_size)\r\n",
        "    lstm_dropout=tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\r\n",
        "    encoder_cell=tf.contrib.rnn.MultiRNNCell([lstm_dropout]*num_layers)\r\n",
        "    encoder_output,encoder_state=tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,   #bidirectional RNN\r\n",
        "                                                  cell_bw=encoder_cell,\r\n",
        "                                                  sequence_length=sequence_length,\r\n",
        "                                                  inputs=rnn_inputs,\r\n",
        "                                                  dtype=tf.float32)\r\n",
        "    return encoder_state \r\n",
        "    \r\n",
        "#decoding the training set #variable scope : advanced data structure to wrap the variables\r\n",
        "def decode_training_set(encoder_state,decoder_cell,decoder_embedded_input,sequence_length,decoding_scope,output_function,keep_prob,batch_size):\r\n",
        "        attention_states=tf.zeros([batch_size, 1, decoder_cell.output_size])\r\n",
        "        attention_keys,attention_values,attention_score_function,attention_construct_function=tf.contrib.seq2seq.prepare_attention(attention_states,\r\n",
        "                                                                                                                           attention_option='bahdanau',\r\n",
        "                                                                                                                           num_units=decoder_cell.output_size)\r\n",
        "        training_decoder_function=tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0],\r\n",
        "                                                                                attention_keys,\r\n",
        "                                                                                attention_values,\r\n",
        "                                                                                attention_score_function,\r\n",
        "                                                                                attention_construct_function,\r\n",
        "                                                                                name='attention_dec_train')\r\n",
        "        decoder_output, decoder_final_state, decoder_final_context_state=tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\r\n",
        "                                                                                                                training_decoder_function,\r\n",
        "                                                                                                                decoder_embedded_input,\r\n",
        "                                                                                                                sequence_length,\r\n",
        "                                                                                                                scope=decoding_scope)\r\n",
        "        decoder_output_dropout=tf.nn.dropout(decoder_output, keep_prob)\r\n",
        "        \r\n",
        "        return output_function(decoder_output_dropout)\r\n",
        "    #decoding of the validation set\r\n",
        "    \r\n",
        "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, keep_prob, batch_size):\r\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\r\n",
        "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"bahdanau\", num_units = decoder_cell.output_size)\r\n",
        "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function,\r\n",
        "                                                                              encoder_state[0],\r\n",
        "                                                                              attention_keys,\r\n",
        "                                                                              attention_values,\r\n",
        "                                                                              attention_score_function,\r\n",
        "                                                                              attention_construct_function,\r\n",
        "                                                                              decoder_embeddings_matrix,\r\n",
        "                                                                              sos_id,\r\n",
        "                                                                              eos_id,\r\n",
        "                                                                              maximum_length,\r\n",
        "                                                                              num_words,\r\n",
        "                                                                              name = \"attn_dec_inf\")\r\n",
        "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell,\r\n",
        "                                                                                                                test_decoder_function,\r\n",
        "                                                                                                                scope = decoding_scope)\r\n",
        "    return test_predictions\r\n",
        "\r\n",
        "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\r\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\r\n",
        "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\r\n",
        "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\r\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([lstm_dropout] * num_layers)\r\n",
        "        weights = tf.truncated_normal_initializer(stddev = 0.1)\r\n",
        "        biases = tf.zeros_initializer()\r\n",
        "        output_function = lambda x: tf.contrib.layers.fully_connected(x,\r\n",
        "                                                                      num_words,\r\n",
        "                                                                      None,\r\n",
        "                                                                      scope = decoding_scope,\r\n",
        "                                                                      weights_initializer = weights,\r\n",
        "                                                                      biases_initializer = biases)\r\n",
        "        training_predictions = decode_training_set(encoder_state,\r\n",
        "                                                   decoder_cell,\r\n",
        "                                                   decoder_embedded_input,\r\n",
        "                                                   sequence_length,\r\n",
        "                                                   decoding_scope,\r\n",
        "                                                   output_function,\r\n",
        "                                                   keep_prob,\r\n",
        "                                                   batch_size)\r\n",
        "        decoding_scope.reuse_variables()\r\n",
        "        test_predictions = decode_test_set(encoder_state,\r\n",
        "                                           decoder_cell,\r\n",
        "                                           decoder_embeddings_matrix,\r\n",
        "                                           word2int['<SOS>'],\r\n",
        "                                           word2int['<EOS>'],\r\n",
        "                                           sequence_length - 1,\r\n",
        "                                           num_words,\r\n",
        "                                           decoding_scope,\r\n",
        "                                           output_function,\r\n",
        "                                           keep_prob,\r\n",
        "                                           batch_size)\r\n",
        "    return training_predictions, test_predictions\r\n",
        "    \r\n",
        "#building the seq2seq model\r\n",
        "def seq2seq_model(inputs,targets,keep_prob,batch_size,sequence_length,answers_num_words,questions_num_words,encoder_embedding_size,decoder_embeddding_size,rnn_size, num_layers,questionswords2int):\r\n",
        "    encoder_embedded_input=tf.contrib.layers.embed_sequence(inputs,\r\n",
        "                                                            answers_num_words+1,\r\n",
        "                                                            encoder_embedding_size,\r\n",
        "                                                            initializer=tf.random_uniform_initializer(0,1))\r\n",
        "    encoder_state=encoder_rnn_layer(encoder_embedded_input, rnn_size,num_layers,keep_prob,sequence_length)\r\n",
        "    preprocessed_targets=preprocess_targets(targets,questionswords2int,batch_size)\r\n",
        "    decoder_embeddings_matrix=tf.Variable(tf.random_uniform([questions_num_words+1,decoder_embeddding_size],0,1)) #takes random numbers from zero and 1 from a uniform distribution\r\n",
        "    decoder_embedded_input=tf.nn.embedding_lookup(decoder_embeddings_matrix,preprocessed_targets)\r\n",
        "    training_predictions,test_predictions=decoder_rnn(decoder_embedded_input,\r\n",
        "                                                      decoder_embeddings_matrix,\r\n",
        "                                                      encoder_state,\r\n",
        "                                                      questions_num_words,\r\n",
        "                                                      sequence_length,\r\n",
        "                                                      rnn_size,\r\n",
        "                                                      num_layers,\r\n",
        "                                                      questionswords2int,\r\n",
        "                                                      keep_prob,\r\n",
        "                                                      batch_size)\r\n",
        "    return training_predictions,test_predictions\r\n",
        "#setting up of the hyperparameters\r\n",
        "epochs=100\r\n",
        "batch_size=2\r\n",
        "rnn_size=256\r\n",
        "num_layers=2\r\n",
        "encoding_embedding_size=512  #512 columns in the embedding matrix\r\n",
        "decoding_embedding_size=256\r\n",
        "learning_rate=0.01\r\n",
        "learning_rate_decay=0.9\r\n",
        "min_learning_rate=0.0001\r\n",
        "keep_probability=0.5\r\n",
        "#defining a session\r\n",
        "tf.reset_default_graph()\r\n",
        "session=tf.InteractiveSession()\r\n",
        "#loading the model inputs\r\n",
        "inputs,targets,lr,keep_prob=model_inputs()\r\n",
        "#setting the sequence_length\r\n",
        "sequence_length=tf.placeholder_with_default(51,None,name='sequence_length')\r\n",
        "\r\n",
        "#getting the shape of the input tensors\r\n",
        "input_shape=tf.shape(inputs)\r\n",
        "#getting the training and testing predictions\r\n",
        "#reverse function reverses the dimensions of the tensors\r\n",
        "training_predictions,test_predictions=seq2seq_model(tf.reverse(inputs,[-1]),\r\n",
        "                                                    targets,\r\n",
        "                                                    keep_prob,\r\n",
        "                                                    batch_size,\r\n",
        "                                                    sequence_length,\r\n",
        "                                                    len(answerswords2int),\r\n",
        "                                                    len(questionswords2int),\r\n",
        "                                                    encoding_embedding_size,\r\n",
        "                                                    decoding_embedding_size,\r\n",
        "                                                    rnn_size,\r\n",
        "                                                    num_layers,\r\n",
        "                                                    questionswords2int)\r\n",
        "    \r\n",
        "    \r\n",
        "#setting the loss error,the optimizer and gradient clippings\r\n",
        "with tf.name_scope('optimization'):\r\n",
        "    loss_error=tf.contrib.seq2seq.sequence_loss(training_predictions,\r\n",
        "                                                targets,\r\n",
        "                                                tf.ones([input_shape[0],sequence_length]))\r\n",
        "    optimizer=tf.train.AdamOptimizer(learning_rate)\r\n",
        "    gradients=optimizer.compute_gradients(loss_error)\r\n",
        "    #gradient clipping\r\n",
        "    clipped_gradients=[(tf.clip_by_value(grad_tensor,-5.0,5.0),grad_variable)for grad_tensor,grad_variable in gradients if grad_tensor is not None]\r\n",
        "    optimizer_gradient_clipping=optimizer.apply_gradients(clipped_gradients)\r\n",
        "\r\n",
        "#padding the sequences to ensure all the questions and answers are equal in size\r\n",
        "def apply_padding(batch_of_sequences,word2int):\r\n",
        "    max_sequence_length=max([len(sequences)for sequences in batch_of_sequences])\r\n",
        "    return [sequence + [word2int['<PAD>']]*(max_sequence_length-len(sequence)) for sequence in batch_of_sequences]\r\n",
        "#spliting the data into batches of questions and answers\r\n",
        "def split_into_batches(questions,answers,batch_size):\r\n",
        "    for batch_index in range(0, len(questions)//batch_size):\r\n",
        "        start_index=batch_index*batch_size\r\n",
        "        questions_in_batch=questions[start_index:start_index+batch_size]\r\n",
        "        answers_in_batch=answers[start_index:start_index+batch_size]\r\n",
        "        padded_questions_in_batch=np.array(apply_padding(questions_in_batch,questionswords2int))\r\n",
        "        padded_answers_in_batch=np.array(apply_padding(answers_in_batch,answerswords2int))\r\n",
        "        yield padded_questions_in_batch,padded_answers_in_batch\r\n",
        "training_validation_split=int(len(sorted_clean_questions)*0.15)\r\n",
        "training_questions=sorted_clean_questions[training_validation_split:]\r\n",
        "training_answers=sorted_clean_answers[training_validation_split:]\r\n",
        "validation_questions=sorted_clean_questions[:training_validation_split]\r\n",
        "_answers=sorted_clean_answers[:training_validation_split]\r\n",
        "#Training\r\n",
        "# Training\r\n",
        "batch_index_check_training_loss = 100\r\n",
        "batch_index_check_validation_loss = ((len(training_questions)) // batch_size // 2) - 1\r\n",
        "total_training_loss_error = 0\r\n",
        "list_validation_loss_error = []\r\n",
        "early_stopping_check = 0\r\n",
        "early_stopping_stop = 1000\r\n",
        "checkpoint = \"chatbot_weights.ckpt\" # For Windows users, replace this line of code by: checkpoint = \"./chatbot_weights.ckpt\"\r\n",
        "session.run(tf.global_variables_initializer())\r\n",
        "for epoch in range(1, epochs + 1):\r\n",
        "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\r\n",
        "        starting_time = time.time()\r\n",
        "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch,\r\n",
        "                                                                                               targets: padded_answers_in_batch,\r\n",
        "                                                                                               lr: learning_rate,\r\n",
        "                                                                                               sequence_length: padded_answers_in_batch.shape[1],\r\n",
        "                                                                                               keep_prob: keep_probability})\r\n",
        "        total_training_loss_error += batch_training_loss_error\r\n",
        "        ending_time = time.time()\r\n",
        "        batch_time = ending_time - starting_time\r\n",
        "        if batch_index % batch_index_check_training_loss == 0:\r\n",
        "            print('Epoch: {:>3}/{}, Batch: {:>4}/{}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds'.format(epoch,\r\n",
        "                                                                                                                                       epochs,\r\n",
        "                                                                                                                                       batch_index,\r\n",
        "                                                                                                                                       len(training_questions) // batch_size,\r\n",
        "                                                                                                                                       total_training_loss_error / batch_index_check_training_loss,\r\n",
        "                                                                                                                                       int(batch_time * batch_index_check_training_loss)))\r\n",
        "            total_training_loss_error = 0\r\n",
        "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\r\n",
        "            total_validation_loss_error = 0\r\n",
        "            starting_time = time.time()\r\n",
        "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\r\n",
        "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\r\n",
        "                                                                       targets: padded_answers_in_batch,\r\n",
        "                                                                       lr: learning_rate,\r\n",
        "                                                                       sequence_length: padded_answers_in_batch.shape[1],\r\n",
        "                                                                       keep_prob: 1})\r\n",
        "                total_validation_loss_error += batch_validation_loss_error\r\n",
        "            ending_time = time.time()\r\n",
        "            batch_time = ending_time - starting_time\r\n",
        "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\r\n",
        "            print('Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds'.format(average_validation_loss_error, int(batch_time)))\r\n",
        "            learning_rate *= learning_rate_decay\r\n",
        "            if learning_rate < min_learning_rate:\r\n",
        "                learning_rate = min_learning_rate\r\n",
        "            list_validation_loss_error.append(average_validation_loss_error)\r\n",
        "            if average_validation_loss_error <= min(list_validation_loss_error):\r\n",
        "                print('I speak better now!!')\r\n",
        "                early_stopping_check = 0\r\n",
        "                saver = tf.train.Saver()\r\n",
        "                saver.save(session, checkpoint)\r\n",
        "            else:\r\n",
        "                print(\"Sorry I do not speak better, I need to practice more.\")\r\n",
        "                early_stopping_check += 1\r\n",
        "                if early_stopping_check == early_stopping_stop:\r\n",
        "                    break\r\n",
        "    if early_stopping_check == early_stopping_stop:\r\n",
        "        print(\"My apologies, I cannot speak better anymore. This is the best I can do.\")\r\n",
        "        break\r\n",
        "print(\"Game Over\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:   1/100, Batch:    0/93143, Training Loss Error:  0.093, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   1/100, Batch:  100/93143, Training Loss Error:  0.549, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch:  200/93143, Training Loss Error:  0.204, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch:  300/93143, Training Loss Error:  0.189, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch:  400/93143, Training Loss Error:  0.168, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch:  500/93143, Training Loss Error:  0.197, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch:  600/93143, Training Loss Error:  0.154, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch:  700/93143, Training Loss Error:  0.164, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch:  800/93143, Training Loss Error:  0.172, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch:  900/93143, Training Loss Error:  0.185, Training Time on 100 Batches: 10 seconds\n",
            "Epoch:   1/100, Batch: 1000/93143, Training Loss Error:  0.147, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 1100/93143, Training Loss Error:  0.199, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 1200/93143, Training Loss Error:  0.162, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 1300/93143, Training Loss Error:  0.178, Training Time on 100 Batches: 10 seconds\n",
            "Epoch:   1/100, Batch: 1400/93143, Training Loss Error:  0.154, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 1500/93143, Training Loss Error:  0.176, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 1600/93143, Training Loss Error:  0.177, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 1700/93143, Training Loss Error:  0.179, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 1800/93143, Training Loss Error:  0.166, Training Time on 100 Batches: 11 seconds\n",
            "Epoch:   1/100, Batch: 1900/93143, Training Loss Error:  0.174, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 2000/93143, Training Loss Error:  0.169, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 2100/93143, Training Loss Error:  0.172, Training Time on 100 Batches: 11 seconds\n",
            "Epoch:   1/100, Batch: 2200/93143, Training Loss Error:  0.166, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 2300/93143, Training Loss Error:  0.157, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 2400/93143, Training Loss Error:  0.162, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   1/100, Batch: 2500/93143, Training Loss Error:  0.169, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 2600/93143, Training Loss Error:  0.191, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 2700/93143, Training Loss Error:  0.190, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 2800/93143, Training Loss Error:  0.186, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 2900/93143, Training Loss Error:  0.170, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 3000/93143, Training Loss Error:  0.162, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 3100/93143, Training Loss Error:  0.203, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 3200/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 3300/93143, Training Loss Error:  0.228, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 3400/93143, Training Loss Error:  0.256, Training Time on 100 Batches: 42 seconds\n",
            "Epoch:   1/100, Batch: 3500/93143, Training Loss Error:  0.213, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch: 3600/93143, Training Loss Error:  0.184, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 3700/93143, Training Loss Error:  0.171, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 3800/93143, Training Loss Error:  0.182, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 3900/93143, Training Loss Error:  0.165, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 4000/93143, Training Loss Error:  0.213, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 4100/93143, Training Loss Error:  0.404, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 4200/93143, Training Loss Error:  0.236, Training Time on 100 Batches: 10 seconds\n",
            "Epoch:   1/100, Batch: 4300/93143, Training Loss Error:  0.182, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 4400/93143, Training Loss Error:  0.175, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 4500/93143, Training Loss Error:  0.190, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 4600/93143, Training Loss Error:  0.213, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 4700/93143, Training Loss Error:  0.237, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 4800/93143, Training Loss Error:  0.363, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 4900/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 5000/93143, Training Loss Error:  0.208, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 5100/93143, Training Loss Error:  0.207, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 5200/93143, Training Loss Error:  0.229, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 5300/93143, Training Loss Error:  0.396, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 5400/93143, Training Loss Error:  0.883, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 5500/93143, Training Loss Error:  0.495, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:   1/100, Batch: 5600/93143, Training Loss Error:  0.317, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 5700/93143, Training Loss Error:  0.269, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 5800/93143, Training Loss Error:  0.318, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 5900/93143, Training Loss Error:  0.225, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 6000/93143, Training Loss Error:  0.264, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 6100/93143, Training Loss Error:  0.236, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 6200/93143, Training Loss Error:  0.302, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 6300/93143, Training Loss Error:  0.240, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 6400/93143, Training Loss Error:  0.180, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 6500/93143, Training Loss Error:  0.269, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 6600/93143, Training Loss Error:  0.209, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 6700/93143, Training Loss Error:  0.312, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 6800/93143, Training Loss Error:  0.178, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 6900/93143, Training Loss Error:  0.204, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:   1/100, Batch: 7000/93143, Training Loss Error:  0.243, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 7100/93143, Training Loss Error:  0.201, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 7200/93143, Training Loss Error:  0.162, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 7300/93143, Training Loss Error:  0.215, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 7400/93143, Training Loss Error:  0.190, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 7500/93143, Training Loss Error:  0.221, Training Time on 100 Batches: 39 seconds\n",
            "Epoch:   1/100, Batch: 7600/93143, Training Loss Error:  0.257, Training Time on 100 Batches: 11 seconds\n",
            "Epoch:   1/100, Batch: 7700/93143, Training Loss Error:  0.183, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 7800/93143, Training Loss Error:  0.163, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 7900/93143, Training Loss Error:  0.176, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 8000/93143, Training Loss Error:  0.296, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 8100/93143, Training Loss Error:  0.171, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 8200/93143, Training Loss Error:  0.185, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 8300/93143, Training Loss Error:  0.711, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch: 8400/93143, Training Loss Error:  0.366, Training Time on 100 Batches: 10 seconds\n",
            "Epoch:   1/100, Batch: 8500/93143, Training Loss Error:  0.450, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 8600/93143, Training Loss Error:  0.228, Training Time on 100 Batches: 130 seconds\n",
            "Epoch:   1/100, Batch: 8700/93143, Training Loss Error:  0.228, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 8800/93143, Training Loss Error:  0.195, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 8900/93143, Training Loss Error:  0.201, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 9000/93143, Training Loss Error:  0.388, Training Time on 100 Batches: 62 seconds\n",
            "Epoch:   1/100, Batch: 9100/93143, Training Loss Error:  0.314, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 9200/93143, Training Loss Error:  0.193, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 9300/93143, Training Loss Error:  0.187, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 9400/93143, Training Loss Error:  0.171, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 9500/93143, Training Loss Error:  0.202, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 9600/93143, Training Loss Error:  0.268, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 9700/93143, Training Loss Error:  0.218, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 9800/93143, Training Loss Error:  0.363, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 9900/93143, Training Loss Error:  0.279, Training Time on 100 Batches: 45 seconds\n",
            "Epoch:   1/100, Batch: 10000/93143, Training Loss Error:  0.246, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 10100/93143, Training Loss Error:  0.215, Training Time on 100 Batches: 10 seconds\n",
            "Epoch:   1/100, Batch: 10200/93143, Training Loss Error:  0.420, Training Time on 100 Batches: 63 seconds\n",
            "Epoch:   1/100, Batch: 10300/93143, Training Loss Error:  0.244, Training Time on 100 Batches: 10 seconds\n",
            "Epoch:   1/100, Batch: 10400/93143, Training Loss Error:  0.200, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 10500/93143, Training Loss Error:  0.202, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 10600/93143, Training Loss Error:  0.179, Training Time on 100 Batches: 43 seconds\n",
            "Epoch:   1/100, Batch: 10700/93143, Training Loss Error:  0.187, Training Time on 100 Batches: 180 seconds\n",
            "Epoch:   1/100, Batch: 10800/93143, Training Loss Error:  0.200, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 10900/93143, Training Loss Error:  0.260, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 11000/93143, Training Loss Error:  0.200, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 11100/93143, Training Loss Error:  0.252, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 11200/93143, Training Loss Error:  0.231, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 11300/93143, Training Loss Error:  0.235, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 11400/93143, Training Loss Error:  0.259, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 11500/93143, Training Loss Error:  0.364, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 11600/93143, Training Loss Error:  0.393, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:   1/100, Batch: 11700/93143, Training Loss Error:  0.274, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 11800/93143, Training Loss Error:  0.493, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 11900/93143, Training Loss Error:  0.667, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 12000/93143, Training Loss Error:  0.399, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 12100/93143, Training Loss Error:  0.258, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 12200/93143, Training Loss Error:  0.239, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 12300/93143, Training Loss Error:  0.213, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 12400/93143, Training Loss Error:  0.230, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 12500/93143, Training Loss Error:  0.387, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 12600/93143, Training Loss Error:  0.234, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 12700/93143, Training Loss Error:  0.212, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 12800/93143, Training Loss Error:  0.328, Training Time on 100 Batches: 55 seconds\n",
            "Epoch:   1/100, Batch: 12900/93143, Training Loss Error:  0.241, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 13000/93143, Training Loss Error:  0.200, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 13100/93143, Training Loss Error:  0.216, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 13200/93143, Training Loss Error:  0.514, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 13300/93143, Training Loss Error:  0.992, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch: 13400/93143, Training Loss Error:  0.723, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 13500/93143, Training Loss Error:  0.591, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 13600/93143, Training Loss Error:  0.214, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 13700/93143, Training Loss Error:  0.209, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 13800/93143, Training Loss Error:  0.198, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 13900/93143, Training Loss Error:  0.207, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 14000/93143, Training Loss Error:  0.199, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 14100/93143, Training Loss Error:  0.251, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 14200/93143, Training Loss Error:  0.268, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 14300/93143, Training Loss Error:  0.350, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 14400/93143, Training Loss Error:  0.384, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 14500/93143, Training Loss Error:  0.322, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 14600/93143, Training Loss Error:  0.263, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 14700/93143, Training Loss Error:  0.231, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 14800/93143, Training Loss Error:  0.196, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 14900/93143, Training Loss Error:  0.416, Training Time on 100 Batches: 57 seconds\n",
            "Epoch:   1/100, Batch: 15000/93143, Training Loss Error:  0.507, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 15100/93143, Training Loss Error:  0.211, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 15200/93143, Training Loss Error:  0.202, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 15300/93143, Training Loss Error:  0.241, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 15400/93143, Training Loss Error:  0.177, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 15500/93143, Training Loss Error:  0.275, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 15600/93143, Training Loss Error:  0.295, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 15700/93143, Training Loss Error:  0.229, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 15800/93143, Training Loss Error:  0.228, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 15900/93143, Training Loss Error:  0.212, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 16000/93143, Training Loss Error:  0.188, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 16100/93143, Training Loss Error:  0.327, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 16200/93143, Training Loss Error:  0.296, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 16300/93143, Training Loss Error:  0.231, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 16400/93143, Training Loss Error:  0.262, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 16500/93143, Training Loss Error:  0.187, Training Time on 100 Batches: 41 seconds\n",
            "Epoch:   1/100, Batch: 16600/93143, Training Loss Error:  0.162, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 16700/93143, Training Loss Error:  0.174, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 16800/93143, Training Loss Error:  0.210, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 16900/93143, Training Loss Error:  0.239, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 17000/93143, Training Loss Error:  0.247, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 17100/93143, Training Loss Error:  0.214, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 17200/93143, Training Loss Error:  0.305, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 17300/93143, Training Loss Error:  0.453, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 17400/93143, Training Loss Error:  0.415, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 17500/93143, Training Loss Error:  0.471, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 17600/93143, Training Loss Error:  1.021, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 17700/93143, Training Loss Error:  0.533, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 17800/93143, Training Loss Error:  0.343, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 17900/93143, Training Loss Error:  0.251, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 18000/93143, Training Loss Error:  0.252, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 18100/93143, Training Loss Error:  0.282, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 18200/93143, Training Loss Error:  0.400, Training Time on 100 Batches: 45 seconds\n",
            "Epoch:   1/100, Batch: 18300/93143, Training Loss Error:  0.418, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 18400/93143, Training Loss Error:  0.319, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 18500/93143, Training Loss Error:  0.259, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 18600/93143, Training Loss Error:  0.364, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 18700/93143, Training Loss Error:  0.202, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 18800/93143, Training Loss Error:  0.171, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 18900/93143, Training Loss Error:  0.197, Training Time on 100 Batches: 65 seconds\n",
            "Epoch:   1/100, Batch: 19000/93143, Training Loss Error:  0.252, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 19100/93143, Training Loss Error:  0.266, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 19200/93143, Training Loss Error:  0.393, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 19300/93143, Training Loss Error:  0.178, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 19400/93143, Training Loss Error:  0.186, Training Time on 100 Batches: 39 seconds\n",
            "Epoch:   1/100, Batch: 19500/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 19600/93143, Training Loss Error:  0.187, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 19700/93143, Training Loss Error:  0.235, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 19800/93143, Training Loss Error:  0.266, Training Time on 100 Batches: 71 seconds\n",
            "Epoch:   1/100, Batch: 19900/93143, Training Loss Error:  0.196, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 20000/93143, Training Loss Error:  0.218, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 20100/93143, Training Loss Error:  0.313, Training Time on 100 Batches: 45 seconds\n",
            "Epoch:   1/100, Batch: 20200/93143, Training Loss Error:  0.472, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 20300/93143, Training Loss Error:  0.203, Training Time on 100 Batches: 62 seconds\n",
            "Epoch:   1/100, Batch: 20400/93143, Training Loss Error:  0.182, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 20500/93143, Training Loss Error:  0.206, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 20600/93143, Training Loss Error:  0.195, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 20700/93143, Training Loss Error:  0.241, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 20800/93143, Training Loss Error:  0.223, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 20900/93143, Training Loss Error:  0.222, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   1/100, Batch: 21000/93143, Training Loss Error:  0.463, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 21100/93143, Training Loss Error:  0.367, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 21200/93143, Training Loss Error:  0.242, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 21300/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 21400/93143, Training Loss Error:  0.179, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 21500/93143, Training Loss Error:  0.208, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 21600/93143, Training Loss Error:  0.292, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 21700/93143, Training Loss Error:  0.287, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 21800/93143, Training Loss Error:  0.166, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 21900/93143, Training Loss Error:  0.168, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 22000/93143, Training Loss Error:  0.165, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 22100/93143, Training Loss Error:  0.194, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 22200/93143, Training Loss Error:  0.288, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 22300/93143, Training Loss Error:  0.394, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 22400/93143, Training Loss Error:  0.221, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 22500/93143, Training Loss Error:  0.177, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 22600/93143, Training Loss Error:  0.187, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 22700/93143, Training Loss Error:  0.193, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 22800/93143, Training Loss Error:  0.213, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 22900/93143, Training Loss Error:  0.331, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 23000/93143, Training Loss Error:  0.225, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 23100/93143, Training Loss Error:  0.202, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 23200/93143, Training Loss Error:  0.472, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 23300/93143, Training Loss Error:  1.033, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   1/100, Batch: 23400/93143, Training Loss Error:  0.506, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 23500/93143, Training Loss Error:  0.287, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 23600/93143, Training Loss Error:  0.213, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 23700/93143, Training Loss Error:  0.215, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 23800/93143, Training Loss Error:  0.361, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 23900/93143, Training Loss Error:  0.239, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 24000/93143, Training Loss Error:  0.449, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:   1/100, Batch: 24100/93143, Training Loss Error:  0.756, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 24200/93143, Training Loss Error:  0.429, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 24300/93143, Training Loss Error:  0.280, Training Time on 100 Batches: 11 seconds\n",
            "Epoch:   1/100, Batch: 24400/93143, Training Loss Error:  0.249, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 24500/93143, Training Loss Error:  0.193, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 24600/93143, Training Loss Error:  0.170, Training Time on 100 Batches: 40 seconds\n",
            "Epoch:   1/100, Batch: 24700/93143, Training Loss Error:  0.198, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   1/100, Batch: 24800/93143, Training Loss Error:  0.171, Training Time on 100 Batches: 149 seconds\n",
            "Epoch:   1/100, Batch: 24900/93143, Training Loss Error:  0.207, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 25000/93143, Training Loss Error:  0.223, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 25100/93143, Training Loss Error:  0.235, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 25200/93143, Training Loss Error:  0.201, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:   1/100, Batch: 25300/93143, Training Loss Error:  0.226, Training Time on 100 Batches: 74 seconds\n",
            "Epoch:   1/100, Batch: 25400/93143, Training Loss Error:  0.163, Training Time on 100 Batches: 184 seconds\n",
            "Epoch:   1/100, Batch: 25500/93143, Training Loss Error:  0.296, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 25600/93143, Training Loss Error:  0.281, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 25700/93143, Training Loss Error:  0.459, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 25800/93143, Training Loss Error:  0.248, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 25900/93143, Training Loss Error:  0.398, Training Time on 100 Batches: 38 seconds\n",
            "Epoch:   1/100, Batch: 26000/93143, Training Loss Error:  0.284, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 26100/93143, Training Loss Error:  0.215, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   1/100, Batch: 26200/93143, Training Loss Error:  0.249, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 26300/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 26400/93143, Training Loss Error:  0.784, Training Time on 100 Batches: 37 seconds\n",
            "Epoch:   1/100, Batch: 26500/93143, Training Loss Error:  0.329, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 26600/93143, Training Loss Error:  0.310, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 26700/93143, Training Loss Error:  0.356, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 26800/93143, Training Loss Error:  0.270, Training Time on 100 Batches: 11 seconds\n",
            "Epoch:   1/100, Batch: 26900/93143, Training Loss Error:  0.368, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 27000/93143, Training Loss Error:  0.386, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 27100/93143, Training Loss Error:  0.232, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   1/100, Batch: 27200/93143, Training Loss Error:  0.266, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 27300/93143, Training Loss Error:  0.248, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 27400/93143, Training Loss Error:  0.194, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 27500/93143, Training Loss Error:  0.227, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 27600/93143, Training Loss Error:  0.235, Training Time on 100 Batches: 46 seconds\n",
            "Epoch:   1/100, Batch: 27700/93143, Training Loss Error:  0.253, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 27800/93143, Training Loss Error:  0.257, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 27900/93143, Training Loss Error:  0.259, Training Time on 100 Batches: 43 seconds\n",
            "Epoch:   1/100, Batch: 28000/93143, Training Loss Error:  0.382, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 28100/93143, Training Loss Error:  0.359, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 28200/93143, Training Loss Error:  0.583, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 28300/93143, Training Loss Error:  0.337, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 28400/93143, Training Loss Error:  0.401, Training Time on 100 Batches: 42 seconds\n",
            "Epoch:   1/100, Batch: 28500/93143, Training Loss Error:  0.324, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 28600/93143, Training Loss Error:  0.216, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 28700/93143, Training Loss Error:  0.194, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 28800/93143, Training Loss Error:  0.230, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 28900/93143, Training Loss Error:  0.374, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 29000/93143, Training Loss Error:  0.244, Training Time on 100 Batches: 78 seconds\n",
            "Epoch:   1/100, Batch: 29100/93143, Training Loss Error:  0.195, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 29200/93143, Training Loss Error:  0.261, Training Time on 100 Batches: 130 seconds\n",
            "Epoch:   1/100, Batch: 29300/93143, Training Loss Error:  0.184, Training Time on 100 Batches: 74 seconds\n",
            "Epoch:   1/100, Batch: 29400/93143, Training Loss Error:  0.233, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 29500/93143, Training Loss Error:  0.223, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   1/100, Batch: 29600/93143, Training Loss Error:  0.185, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 29700/93143, Training Loss Error:  0.435, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 29800/93143, Training Loss Error:  0.692, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 29900/93143, Training Loss Error:  0.548, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 30000/93143, Training Loss Error:  0.410, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 30100/93143, Training Loss Error:  0.465, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 30200/93143, Training Loss Error:  0.280, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:   1/100, Batch: 30300/93143, Training Loss Error:  0.238, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 30400/93143, Training Loss Error:  0.249, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 30500/93143, Training Loss Error:  0.437, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 30600/93143, Training Loss Error:  0.768, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 30700/93143, Training Loss Error:  0.614, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 30800/93143, Training Loss Error:  0.466, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 30900/93143, Training Loss Error:  0.493, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 31000/93143, Training Loss Error:  0.273, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 31100/93143, Training Loss Error:  0.215, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 31200/93143, Training Loss Error:  0.172, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 31300/93143, Training Loss Error:  0.211, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 31400/93143, Training Loss Error:  0.249, Training Time on 100 Batches: 137 seconds\n",
            "Epoch:   1/100, Batch: 31500/93143, Training Loss Error:  0.362, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 31600/93143, Training Loss Error:  0.361, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 31700/93143, Training Loss Error:  0.276, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 31800/93143, Training Loss Error:  0.630, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 31900/93143, Training Loss Error:  0.362, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 32000/93143, Training Loss Error:  0.667, Training Time on 100 Batches: 41 seconds\n",
            "Epoch:   1/100, Batch: 32100/93143, Training Loss Error:  0.348, Training Time on 100 Batches: 66 seconds\n",
            "Epoch:   1/100, Batch: 32200/93143, Training Loss Error:  0.237, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 32300/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 42 seconds\n",
            "Epoch:   1/100, Batch: 32400/93143, Training Loss Error:  0.221, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 32500/93143, Training Loss Error:  0.374, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 32600/93143, Training Loss Error:  0.357, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 32700/93143, Training Loss Error:  0.495, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 32800/93143, Training Loss Error:  0.370, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 32900/93143, Training Loss Error:  0.267, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 33000/93143, Training Loss Error:  0.378, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 33100/93143, Training Loss Error:  0.546, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 33200/93143, Training Loss Error:  0.259, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 33300/93143, Training Loss Error:  0.312, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 33400/93143, Training Loss Error:  0.233, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 33500/93143, Training Loss Error:  0.204, Training Time on 100 Batches: 81 seconds\n",
            "Epoch:   1/100, Batch: 33600/93143, Training Loss Error:  0.185, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 33700/93143, Training Loss Error:  0.541, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:   1/100, Batch: 33800/93143, Training Loss Error:  0.497, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 33900/93143, Training Loss Error:  0.278, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 34000/93143, Training Loss Error:  0.261, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   1/100, Batch: 34100/93143, Training Loss Error:  0.253, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 34200/93143, Training Loss Error:  0.443, Training Time on 100 Batches: 83 seconds\n",
            "Epoch:   1/100, Batch: 34300/93143, Training Loss Error:  0.273, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 34400/93143, Training Loss Error:  0.290, Training Time on 100 Batches: 70 seconds\n",
            "Epoch:   1/100, Batch: 34500/93143, Training Loss Error:  0.230, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 34600/93143, Training Loss Error:  0.167, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 34700/93143, Training Loss Error:  0.193, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 34800/93143, Training Loss Error:  0.203, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 34900/93143, Training Loss Error:  0.155, Training Time on 100 Batches: 44 seconds\n",
            "Epoch:   1/100, Batch: 35000/93143, Training Loss Error:  0.166, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 35100/93143, Training Loss Error:  0.282, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 35200/93143, Training Loss Error:  0.180, Training Time on 100 Batches: 37 seconds\n",
            "Epoch:   1/100, Batch: 35300/93143, Training Loss Error:  0.186, Training Time on 100 Batches: 54 seconds\n",
            "Epoch:   1/100, Batch: 35400/93143, Training Loss Error:  0.514, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 35500/93143, Training Loss Error:  0.940, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 35600/93143, Training Loss Error:  0.706, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 35700/93143, Training Loss Error:  0.329, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 35800/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 35900/93143, Training Loss Error:  0.210, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 36000/93143, Training Loss Error:  0.177, Training Time on 100 Batches: 45 seconds\n",
            "Epoch:   1/100, Batch: 36100/93143, Training Loss Error:  0.167, Training Time on 100 Batches: 37 seconds\n",
            "Epoch:   1/100, Batch: 36200/93143, Training Loss Error:  0.181, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 36300/93143, Training Loss Error:  0.159, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 36400/93143, Training Loss Error:  0.207, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 36500/93143, Training Loss Error:  0.199, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 36600/93143, Training Loss Error:  0.168, Training Time on 100 Batches: 14 seconds\n",
            "Epoch:   1/100, Batch: 36700/93143, Training Loss Error:  0.176, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 36800/93143, Training Loss Error:  0.176, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 36900/93143, Training Loss Error:  0.184, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 37000/93143, Training Loss Error:  0.424, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 37100/93143, Training Loss Error:  0.339, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 37200/93143, Training Loss Error:  0.248, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 37300/93143, Training Loss Error:  0.180, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 37400/93143, Training Loss Error:  0.211, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 37500/93143, Training Loss Error:  0.176, Training Time on 100 Batches: 34 seconds\n",
            "Epoch:   1/100, Batch: 37600/93143, Training Loss Error:  0.202, Training Time on 100 Batches: 11 seconds\n",
            "Epoch:   1/100, Batch: 37700/93143, Training Loss Error:  0.270, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 37800/93143, Training Loss Error:  0.561, Training Time on 100 Batches: 12 seconds\n",
            "Epoch:   1/100, Batch: 37900/93143, Training Loss Error:  0.603, Training Time on 100 Batches: 91 seconds\n",
            "Epoch:   1/100, Batch: 38000/93143, Training Loss Error:  0.523, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 38100/93143, Training Loss Error:  0.293, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 38200/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 38300/93143, Training Loss Error:  0.184, Training Time on 100 Batches: 93 seconds\n",
            "Epoch:   1/100, Batch: 38400/93143, Training Loss Error:  0.165, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 38500/93143, Training Loss Error:  0.174, Training Time on 100 Batches: 49 seconds\n",
            "Epoch:   1/100, Batch: 38600/93143, Training Loss Error:  0.212, Training Time on 100 Batches: 50 seconds\n",
            "Epoch:   1/100, Batch: 38700/93143, Training Loss Error:  0.193, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 38800/93143, Training Loss Error:  0.203, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 38900/93143, Training Loss Error:  0.167, Training Time on 100 Batches: 40 seconds\n",
            "Epoch:   1/100, Batch: 39000/93143, Training Loss Error:  0.177, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 39100/93143, Training Loss Error:  0.244, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 39200/93143, Training Loss Error:  0.259, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 39300/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 33 seconds\n",
            "Epoch:   1/100, Batch: 39400/93143, Training Loss Error:  0.231, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 39500/93143, Training Loss Error:  0.525, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 39600/93143, Training Loss Error:  0.551, Training Time on 100 Batches: 22 seconds\n",
            "Epoch:   1/100, Batch: 39700/93143, Training Loss Error:  0.355, Training Time on 100 Batches: 31 seconds\n",
            "Epoch:   1/100, Batch: 39800/93143, Training Loss Error:  0.285, Training Time on 100 Batches: 20 seconds\n",
            "Epoch:   1/100, Batch: 39900/93143, Training Loss Error:  0.207, Training Time on 100 Batches: 52 seconds\n",
            "Epoch:   1/100, Batch: 40000/93143, Training Loss Error:  0.196, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 40100/93143, Training Loss Error:  0.217, Training Time on 100 Batches: 15 seconds\n",
            "Epoch:   1/100, Batch: 40200/93143, Training Loss Error:  0.193, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 40300/93143, Training Loss Error:  0.233, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 40400/93143, Training Loss Error:  0.225, Training Time on 100 Batches: 35 seconds\n",
            "Epoch:   1/100, Batch: 40500/93143, Training Loss Error:  0.199, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 40600/93143, Training Loss Error:  0.193, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 40700/93143, Training Loss Error:  0.339, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 40800/93143, Training Loss Error:  0.705, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 40900/93143, Training Loss Error:  0.640, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 41000/93143, Training Loss Error:  0.587, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 41100/93143, Training Loss Error:  0.420, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 41200/93143, Training Loss Error:  0.252, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 41300/93143, Training Loss Error:  0.197, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 41400/93143, Training Loss Error:  0.333, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 41500/93143, Training Loss Error:  0.206, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 41600/93143, Training Loss Error:  0.186, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 41700/93143, Training Loss Error:  0.235, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 41800/93143, Training Loss Error:  0.219, Training Time on 100 Batches: 47 seconds\n",
            "Epoch:   1/100, Batch: 41900/93143, Training Loss Error:  0.204, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 42000/93143, Training Loss Error:  0.292, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 42100/93143, Training Loss Error:  0.199, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 42200/93143, Training Loss Error:  0.259, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 42300/93143, Training Loss Error:  0.469, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   1/100, Batch: 42400/93143, Training Loss Error:  0.321, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 42500/93143, Training Loss Error:  0.309, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:   1/100, Batch: 42600/93143, Training Loss Error:  0.392, Training Time on 100 Batches: 19 seconds\n",
            "Epoch:   1/100, Batch: 42700/93143, Training Loss Error:  0.289, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 42800/93143, Training Loss Error:  0.221, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 42900/93143, Training Loss Error:  0.262, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 43000/93143, Training Loss Error:  0.180, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 43100/93143, Training Loss Error:  0.155, Training Time on 100 Batches: 58 seconds\n",
            "Epoch:   1/100, Batch: 43200/93143, Training Loss Error:  0.175, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 43300/93143, Training Loss Error:  0.173, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 43400/93143, Training Loss Error:  0.177, Training Time on 100 Batches: 45 seconds\n",
            "Epoch:   1/100, Batch: 43500/93143, Training Loss Error:  0.208, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 43600/93143, Training Loss Error:  0.177, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 43700/93143, Training Loss Error:  0.160, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 43800/93143, Training Loss Error:  0.172, Training Time on 100 Batches: 25 seconds\n",
            "Epoch:   1/100, Batch: 43900/93143, Training Loss Error:  0.255, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 44000/93143, Training Loss Error:  0.577, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 44100/93143, Training Loss Error:  1.320, Training Time on 100 Batches: 30 seconds\n",
            "Epoch:   1/100, Batch: 44200/93143, Training Loss Error:  0.745, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 44300/93143, Training Loss Error:  0.689, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 44400/93143, Training Loss Error:  0.533, Training Time on 100 Batches: 138 seconds\n",
            "Epoch:   1/100, Batch: 44500/93143, Training Loss Error:  0.406, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 44600/93143, Training Loss Error:  0.315, Training Time on 100 Batches: 18 seconds\n",
            "Epoch:   1/100, Batch: 44700/93143, Training Loss Error:  0.249, Training Time on 100 Batches: 79 seconds\n",
            "Epoch:   1/100, Batch: 44800/93143, Training Loss Error:  0.249, Training Time on 100 Batches: 36 seconds\n",
            "Epoch:   1/100, Batch: 44900/93143, Training Loss Error:  0.280, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 45000/93143, Training Loss Error:  0.262, Training Time on 100 Batches: 17 seconds\n",
            "Epoch:   1/100, Batch: 45100/93143, Training Loss Error:  0.214, Training Time on 100 Batches: 26 seconds\n",
            "Epoch:   1/100, Batch: 45200/93143, Training Loss Error:  0.235, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 45300/93143, Training Loss Error:  0.214, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 45400/93143, Training Loss Error:  0.263, Training Time on 100 Batches: 21 seconds\n",
            "Epoch:   1/100, Batch: 45500/93143, Training Loss Error:  0.245, Training Time on 100 Batches: 43 seconds\n",
            "Epoch:   1/100, Batch: 45600/93143, Training Loss Error:  0.181, Training Time on 100 Batches: 23 seconds\n",
            "Epoch:   1/100, Batch: 45700/93143, Training Loss Error:  0.230, Training Time on 100 Batches: 24 seconds\n",
            "Epoch:   1/100, Batch: 45800/93143, Training Loss Error:  0.375, Training Time on 100 Batches: 16 seconds\n",
            "Epoch:   1/100, Batch: 45900/93143, Training Loss Error:  0.360, Training Time on 100 Batches: 28 seconds\n",
            "Epoch:   1/100, Batch: 46000/93143, Training Loss Error:  0.375, Training Time on 100 Batches: 13 seconds\n",
            "Epoch:   1/100, Batch: 46100/93143, Training Loss Error:  0.283, Training Time on 100 Batches: 53 seconds\n",
            "Epoch:   1/100, Batch: 46200/93143, Training Loss Error:  0.224, Training Time on 100 Batches: 27 seconds\n",
            "Epoch:   1/100, Batch: 46300/93143, Training Loss Error:  0.170, Training Time on 100 Batches: 32 seconds\n",
            "Epoch:   1/100, Batch: 46400/93143, Training Loss Error:  0.187, Training Time on 100 Batches: 29 seconds\n",
            "Epoch:   1/100, Batch: 46500/93143, Training Loss Error:  0.172, Training Time on 100 Batches: 40 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-63b42c4bac82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mtotal_validation_loss_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mstarting_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpadded_questions_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_answers_in_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_into_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_answers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m                 batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch,\n\u001b[1;32m    359\u001b[0m                                                                        \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpadded_answers_in_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'validation_answers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "qcDkRQI7gmgO",
        "outputId": "39fe2f69-0f00-49db-a664-8b8254ad0988"
      },
      "source": [
        "!pip install tensorflow==1.0.0\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/58/b71480f9ec9d08d581d672a81b15ab5fec36a5fcda2093558a23614d8468/tensorflow-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (44.5MB)\n",
            "\u001b[K     || 44.5MB 115kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (0.36.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.1.0->tensorflow==1.0.0) (50.3.2)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed tensorflow-1.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6Le2dArfG9u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c705d92-f162-4016-a90e-534375c94997"
      },
      "source": [
        " \r\n",
        " \r\n",
        "# Loading the weights and Running the session\r\n",
        "checkpoint = \"./chatbot_weights.ckpt\"\r\n",
        "session = tf.InteractiveSession()\r\n",
        "session.run(tf.global_variables_initializer())\r\n",
        "saver = tf.train.Saver()\r\n",
        "saver.restore(session, checkpoint)\r\n",
        " \r\n",
        "# Converting the questions from strings to lists of encoding integers\r\n",
        "def convert_string2int(question, word2int):\r\n",
        "    question = clean_text(question)\r\n",
        "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]\r\n",
        " \r\n",
        "# Setting up the chat\r\n",
        "while(True):\r\n",
        "    question = input(\"You: \")\r\n",
        "    if question == 'Goodbye':\r\n",
        "        break\r\n",
        "    question = convert_string2int(question, questionswords2int)\r\n",
        "    question = question + [questionswords2int['<PAD>']] * (25 - len(question))\r\n",
        "    fake_batch = np.zeros((batch_size, 25))\r\n",
        "    fake_batch[0] = question\r\n",
        "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\r\n",
        "    answer = ''\r\n",
        "    for i in np.argmax(predicted_answer, 1):\r\n",
        "        if answersints2word[i] == 'i':\r\n",
        "            token = ' I'\r\n",
        "        elif answersints2word[i] == '<EOS>':\r\n",
        "            token = '.'\r\n",
        "        elif answersints2word[i] == '<OUT>':\r\n",
        "            token = 'out'\r\n",
        "        else:\r\n",
        "            token = ' ' + answersints2word[i]\r\n",
        "        answer += token\r\n",
        "        if token == '.':\r\n",
        "            break\r\n",
        "    print('ChatBot: ' + answer)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./chatbot_weights.ckpt\n\t [[Node: save/RestoreV2_27 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_27/tensor_names, save/RestoreV2_27/shape_and_slices)]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bd68becfe428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Converting the questions from strings to lists of encoding integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1437\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1439\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./chatbot_weights.ckpt\n\t [[Node: save/RestoreV2_27 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_27/tensor_names, save/RestoreV2_27/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_27', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.6/asyncio/base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.6/asyncio/events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 462, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 492, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 444, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-bd68becfe428>\", line 7, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1051, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1081, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./chatbot_weights.ckpt\n\t [[Node: save/RestoreV2_27 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_27/tensor_names, save/RestoreV2_27/shape_and_slices)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "vOFMwQY9gGB_",
        "outputId": "098fe611-03f5-4f5a-fc71-e8b373fafe7a"
      },
      "source": [
        "\\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-22f10141aa0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'disable_v2_behavior'"
          ]
        }
      ]
    }
  ]
}