{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Model_construct",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOp8EMb8uLK5QwOWnFc96c2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Satyake/NLP/blob/master/NLP_Model_construct(Sentiment%20Analysis-PreDeployment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JyAlBmrCU3BY",
        "outputId": "d73490e1-ed42-4c12-a696-514649980c2d"
      },
      "source": [
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import nltk\r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import pickle\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from tensorflow.keras.layers import Embedding,LeakyReLU\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.preprocessing.text import one_hot\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "data_train=pd.read_csv('/content/train.csv',encoding='ISO-8859-1')\r\n",
        "data_test=pd.read_csv('/content/test.csv',encoding='ISO-8859-1')\r\n",
        "LE=LabelEncoder()\r\n",
        "x=data_train.iloc[:,[2]].values\r\n",
        "y=data_train.iloc[:,[3]].values\r\n",
        "\r\n",
        "y=LE.fit_transform(y)\r\n",
        "x=x.tolist()\r\n",
        "LEMMATIZER=WordNetLemmatizer()\r\n",
        "corpus=[]\r\n",
        "for i in range(0,len(x)):\r\n",
        "    lines=re.sub('[^a-zA-Z]',' ',str(x[i]))\r\n",
        "    #lines=line.sub('https?:\\/\\/.*[\\r\\n]*','',lines)\r\n",
        "    lines=lines.lower()\r\n",
        "    lines=lines.split()\r\n",
        "    lines=[LEMMATIZER.lemmatize(j) for j in lines if j not in stopwords.words('english')]\r\n",
        "    lines=' '.join(lines)\r\n",
        "    corpus.append(lines)\r\n",
        "len(corpus)\r\n",
        "sentence_length=50\r\n",
        "OHR=[one_hot(k,4000) for k in corpus]\r\n",
        "embeddings=pad_sequences(OHR,sentence_length)\r\n",
        "\r\n",
        "x_train,x_test,y_train,y_test=train_test_split(embeddings,y,train_size=0.7,shuffle=True)\r\n",
        "\r\n",
        "\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM,Bidirectional\r\n",
        "model=Sequential()\r\n",
        "model.add(Embedding(4000,20,input_length=sentence_length))\r\n",
        "model.add(Bidirectional(LSTM(30,activation='tanh',return_sequences=True)))\r\n",
        "model.add(LeakyReLU(alpha=0.3))\r\n",
        "model.add(LSTM(20,activation='tanh',return_sequences=False))\r\n",
        "model.add(Dense(5,activation='softmax'))\r\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=40,batch_size=15)\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "7283/7283 [==============================] - 76s 10ms/step - loss: 1.1617 - accuracy: 0.5386 - val_loss: 1.0044 - val_accuracy: 0.5937\n",
            "Epoch 2/40\n",
            "7283/7283 [==============================] - 69s 9ms/step - loss: 0.9610 - accuracy: 0.6135 - val_loss: 0.9643 - val_accuracy: 0.6094\n",
            "Epoch 3/40\n",
            "7283/7283 [==============================] - 69s 9ms/step - loss: 0.9033 - accuracy: 0.6371 - val_loss: 0.9482 - val_accuracy: 0.6161\n",
            "Epoch 4/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.8650 - accuracy: 0.6529 - val_loss: 0.9413 - val_accuracy: 0.6187\n",
            "Epoch 5/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.8270 - accuracy: 0.6679 - val_loss: 0.9374 - val_accuracy: 0.6208\n",
            "Epoch 6/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.8052 - accuracy: 0.6770 - val_loss: 0.9372 - val_accuracy: 0.6260\n",
            "Epoch 7/40\n",
            "7283/7283 [==============================] - 69s 9ms/step - loss: 0.7795 - accuracy: 0.6875 - val_loss: 0.9350 - val_accuracy: 0.6282\n",
            "Epoch 8/40\n",
            "7283/7283 [==============================] - 71s 10ms/step - loss: 0.7591 - accuracy: 0.6948 - val_loss: 0.9398 - val_accuracy: 0.6238\n",
            "Epoch 9/40\n",
            "7283/7283 [==============================] - 70s 10ms/step - loss: 0.7393 - accuracy: 0.7047 - val_loss: 0.9525 - val_accuracy: 0.6250\n",
            "Epoch 10/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.7189 - accuracy: 0.7117 - val_loss: 0.9467 - val_accuracy: 0.6313\n",
            "Epoch 11/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.7054 - accuracy: 0.7186 - val_loss: 0.9567 - val_accuracy: 0.6302\n",
            "Epoch 12/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.6908 - accuracy: 0.7238 - val_loss: 0.9638 - val_accuracy: 0.6293\n",
            "Epoch 13/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.6742 - accuracy: 0.7318 - val_loss: 0.9662 - val_accuracy: 0.6310\n",
            "Epoch 14/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.6607 - accuracy: 0.7357 - val_loss: 0.9809 - val_accuracy: 0.6263\n",
            "Epoch 15/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.6447 - accuracy: 0.7414 - val_loss: 0.9894 - val_accuracy: 0.6271\n",
            "Epoch 16/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.6325 - accuracy: 0.7460 - val_loss: 1.0106 - val_accuracy: 0.6271\n",
            "Epoch 17/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.6234 - accuracy: 0.7492 - val_loss: 1.0062 - val_accuracy: 0.6225\n",
            "Epoch 18/40\n",
            "7283/7283 [==============================] - 68s 9ms/step - loss: 0.6107 - accuracy: 0.7561 - val_loss: 1.0233 - val_accuracy: 0.6241\n",
            "Epoch 19/40\n",
            "7277/7283 [============================>.] - ETA: 0s - loss: 0.6065 - accuracy: 0.7573"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-688128087bf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz4s6YnE8dHu",
        "outputId": "b51cf0f4-1a4e-496e-899e-d24cc63a08a4"
      },
      "source": [
        "import pandas as pd \r\n",
        "import numpy as np \r\n",
        "import nltk\r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "import pickle\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from tensorflow.keras.layers import Embedding,LeakyReLU\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.preprocessing.text import one_hot\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "data=pd.read_csv('all-data.csv',encoding='ISO-8859-1')\r\n",
        "LE=LabelEncoder()\r\n",
        "y=data.iloc[:,[0]].values\r\n",
        "x=data.iloc[:,[1]].values\r\n",
        "y=LE.fit_transform(y)\r\n",
        "x=x.tolist()\r\n",
        "LEMMATIZER=WordNetLemmatizer()\r\n",
        "corpus=[]\r\n",
        "for i in range(0,len(x)):\r\n",
        "    lines=re.sub('[^a-zA-Z]',' ',str(x[i]))\r\n",
        "    #lines=line.sub('https?:\\/\\/.*[\\r\\n]*','',lines)\r\n",
        "    lines=lines.lower()\r\n",
        "    lines=lines.split()\r\n",
        "    lines=[LEMMATIZER.lemmatize(j) for j in lines if j not in stopwords.words('english')]\r\n",
        "    lines=' '.join(lines)\r\n",
        "    corpus.append(lines)\r\n",
        "len(corpus)\r\n",
        "vectors=Word2Vec(corpus)\r\n",
        "sentence_length=80\r\n",
        "OHR=[one_hot(k,4000) for k in corpus]\r\n",
        "embeddings=pad_sequences(OHR,sentence_length)\r\n",
        "GNB=GaussianNB()\r\n",
        "\r\n",
        "x_train,x_test,y_train,y_test=train_test_split(embeddings,y,train_size=0.7,shuffle=True)\r\n",
        "GNB.fit(x_train,y_train)\r\n",
        "predicted=GNB.predict(x_test)\r\n",
        "RFC=RandomForestClassifier()\r\n",
        "RFC.fit(x_train,y_train)\r\n",
        "predicted_rf=RFC.predict(x_test)\r\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\r\n",
        "accuracy_score(predicted_rf,y_test)\r\n",
        "\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import LSTM,Bidirectional\r\n",
        "model=Sequential()\r\n",
        "model.add(Embedding(4000,30,input_length=sentence_length))\r\n",
        "model.add(Bidirectional(LSTM(300,activation='relu',return_sequences=True)))\r\n",
        "model.add(LeakyReLU(alpha=0.3))\r\n",
        "model.add(LSTM(200,activation='relu',return_sequences=False))\r\n",
        "model.add(Dense(3,activation='softmax'))\r\n",
        "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=40,batch_size=15)\r\n",
        "\r\n",
        "\r\n",
        "model.save('NLPTrained.h5')\r\n",
        "corpus1=[]\r\n",
        "preds=model.predict(x_test)\r\n",
        "preds= np.argmax(preds,axis=-1)\r\n",
        "accuracy_score(preds,y_test)\r\n",
        "confusion_matrix(preds,y_test)\r\n",
        "\r\n",
        "unknown='Alas '\r\n",
        "unknown=np.array(unknown)\r\n",
        "lines1=re.sub('[^a-zA-Z]',' ',str(unknown))\r\n",
        "      #lines=line.sub('https?:\\/\\/.*[\\r\\n]*','',lines)\r\n",
        "lines1=lines1.lower()\r\n",
        "lines1=lines1.split()\r\n",
        "lines1=[LEMMATIZER.lemmatize(j) for j in lines1 if j not in stopwords.words('english')]\r\n",
        "lines1=' '.join(lines1)\r\n",
        "corpus1.append(lines1)\r\n",
        "OHR=[one_hot(k,4000) for k in corpus1]\r\n",
        "embeddings=pad_sequences(OHR,sentence_length)\r\n",
        "embeddings=embeddings.reshape(-1,1)\r\n",
        "embeddings=np.transpose(embeddings)\r\n",
        "predicted=model.predict(embeddings)\r\n",
        "np.argmax(predicted,axis=-1)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Epoch 1/40\n",
            "221/227 [============================>.] - ETA: 1s - loss: nan - accuracy: 0.5677"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4ovWAcU8iUk",
        "outputId": "e21e66dc-96c2-4c3e-95a2-4c442acc174e"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YikKjVtOguv8",
        "outputId": "03f88342-1720-45fe-c69c-cf8f6696318a"
      },
      "source": [
        "x_train,x_test,y_train,y_test=train_test_split(embeddings,y,train_size=0.7,shuffle=True)\r\n",
        "GNB.fit(x_train,y_train)\r\n",
        "from sklearn.svm import SVC\r\n",
        "SVM=SVC()\r\n",
        "SVM.fit(x_train,y_train)\r\n",
        "pred_svm=SVM.predict(x_test)\r\n",
        "predicted=GNB.predict(x_test)\r\n",
        "RFC=RandomForestClassifier()\r\n",
        "RFC.fit(x_train,y_train)\r\n",
        "predicted_rf=RFC.predict(x_test)\r\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\r\n",
        "accuracy_score(predicted_rf,y_test)\r\n",
        "confusion_matrix(predicted,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[182, 829, 401],\n",
              "       [  0,   2,   0],\n",
              "       [  5,  24,  11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNubG9xQdM_4",
        "outputId": "7e6ebea5-2a4f-46cd-9962-b21e9f7bf952"
      },
      "source": [
        "model.save('NLPTrained.h5')\r\n",
        "corpus1=[]\r\n",
        "preds=model.predict(x_test)\r\n",
        "preds= np.argmax(preds,axis=-1)\r\n",
        "accuracy_score(preds,y_test)\r\n",
        "confusion_matrix(preds,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 11,   2,  12],\n",
              "       [ 71, 702, 208],\n",
              "       [ 97, 141, 210]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsCEhexrgsbj",
        "outputId": "f89c16b5-3947-4c2d-cfe2-b521d8e33842"
      },
      "source": [
        "x_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ..., 2975, 1978, 2975],\n",
              "       [   0,    0,    0, ...,   60, 3789, 2878],\n",
              "       [   0,    0,    0, ...,  152, 1025, 2060],\n",
              "       ...,\n",
              "       [   0,    0,    0, ..., 3607, 3223,   60],\n",
              "       [   0,    0,    0, ..., 2659, 1497, 3307],\n",
              "       [   0,    0,    0, ..., 1403,  189, 2735]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9f0rn-ZX3d9",
        "outputId": "c3a2717b-334b-4c44-a826-807fa98f1ea9"
      },
      "source": [
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}